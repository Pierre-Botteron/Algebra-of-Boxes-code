{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import non_local_boxes\n",
    "import numpy as np\n",
    "\n",
    "# Sugar coating for reloading\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# in ordert to have unblurred pictures\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_local_boxes.evaluate.nb_columns = int(1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This notebook plots the histograms of the ouputs of `Algorithms 2` and `3`. In the manuscript, it is useful espacially for Figure 9.\n",
    ">\n",
    ">Standard reference in Numerical Optimization: https://doi.org/10.1007/b98874"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# I. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{l}\n",
    "    x_{k+1}= \\texttt{proj}(x_k + \\alpha \\nabla \\phi(x_k))\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_wiring = non_local_boxes.utils.projected_wiring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(starting_W, P, Q, learning_rate, nb_iterations = 400, tolerance=1e-6):\n",
    "    m = non_local_boxes.evaluate.nb_columns\n",
    "    external_grad = torch.ones(m)\n",
    "    W = starting_W\n",
    "    for _ in range(nb_iterations):\n",
    "        Wold = W\n",
    "        non_local_boxes.evaluate.phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "        W = projected_wiring(W + learning_rate*W.grad).detach() \n",
    "        if (torch.max(torch.abs(W-Wold)) < tolerance):   return W\n",
    "        W.requires_grad=True\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of the Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR = non_local_boxes.utils.PR\n",
    "SR = non_local_boxes.utils.SR\n",
    "I = non_local_boxes.utils.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.39\n",
    "q=0.6\n",
    "P = p*PR +q*SR + (1-p-q)*I\n",
    "BoxProduct = non_local_boxes.evaluate.phi_flat\n",
    "\n",
    "m = non_local_boxes.evaluate.nb_columns\n",
    "alpha = 0.01   # 0.01\n",
    "K=int(1e2)     # int(1e2)\n",
    "epsilon=1e-6   # 1e-6\n",
    "\n",
    "W = gradient_descent(\n",
    "    starting_W=non_local_boxes.utils.random_wiring(m),\n",
    "    P=P, Q=P,\n",
    "    learning_rate=alpha, nb_iterations=K, tolerance=epsilon\n",
    ")\n",
    "histogramGD = BoxProduct(W, P, P).tolist()\n",
    "\n",
    "plt.hist(histogramGD, bins=22, \n",
    "         label=f\"\"\"Projected Gradient Descent\\n(α={alpha}, K=$10^{{{int(np.log10(K))}}}$, ε=$10^{{{int(np.log10(epsilon))}}}$, m=$10^{{{int(np.log10(m))}}}$).\"\"\")\n",
    "plt.xlim(0.3, 1)\n",
    "plt.ylim(0.9, m)\n",
    "plt.xlabel(\"$\\Phi(\\mathsf{W}_{{out}})$\", fontsize=13)\n",
    "plt.ylabel(\"Number of reruns\", fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# II. Line Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left\\{\n",
    "\\begin{array}{l}\n",
    "    \\alpha^*_k = \\argmax_\\alpha \\phi(x_k + \\alpha \\nabla \\phi(x_k))\\\\\n",
    "    x_{k+1}= \\texttt{proj}(x_k + \\alpha^*_k \\nabla \\phi(x_k))\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_list(L, phi):\n",
    "    j=0\n",
    "    while j<len(L):\n",
    "        if j!=0 and phi[L[j-1]]<phi[L[j]]:\n",
    "            L[j-1],L[j]=L[j],L[j-1]\n",
    "            j-=2\n",
    "        j+=1\n",
    "    return L\n",
    "\n",
    "# example of use:\n",
    "phi=[0.1, 0.3, 0, 10, 9, 0.5]\n",
    "L = [*range(len(phi))]\n",
    "L = reorder_list(L, phi)\n",
    "print([phi[k] for k in L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_columns(W, P, Q, integer):\n",
    "    if integer==0: return non_local_boxes.utils.random_wiring(m).detach()\n",
    "    # L is the list of the \"best\" indexes of the columns of W\n",
    "    # At the begining, we take the first indexes of W\n",
    "    # We will change the list L by comparing the value at the other indexes\n",
    "    # When we add a term to L, we also remove the \"worst\" one, and we re-order the list L\n",
    "    L = [*range(integer)]\n",
    "    # phi is the list of values:\n",
    "    phi= non_local_boxes.evaluate.phi_flat(W,P,Q).tolist()\n",
    "    # We re-order the list L:\n",
    "    L = reorder_list(L, phi)\n",
    "    for i in range(integer,non_local_boxes.evaluate.nb_columns):\n",
    "        if phi[i]>phi[L[-1]]:\n",
    "            L[-1]=i # We remove and replace the worst index\n",
    "            L = reorder_list(L, phi)\n",
    "\n",
    "    W_new = non_local_boxes.utils.random_wiring(m).detach()\n",
    "    for k in range(integer): W_new[:,L[k]] = W[:,L[k]] # We keep only the best ones\n",
    "\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search_with_resets(P, Q, LS_iterations, K_reset, chi):\n",
    "    # P,Q are 4x4 matrices\n",
    "    m = non_local_boxes.evaluate.nb_columns\n",
    "    phi_flat = non_local_boxes.evaluate.phi_flat\n",
    "    W, external_grad = torch.zeros(32,m), torch.ones(m)\n",
    "    Krange, LSrange = range(K_reset), range(LS_iterations)\n",
    "    \n",
    "    for j in range(0,int(1/chi)):\n",
    "        # Reset some of the wirings:\n",
    "        W = select_best_columns(W, P, Q, min(m, int(j*m*chi))).detach()\n",
    "        W.requires_grad=True\n",
    "\n",
    "        # At the end, we do a lot of steps:\n",
    "        if j==int(1/chi)-1:  Krange=range(10*K_reset)\n",
    "\n",
    "        # Line search:\n",
    "        for _ in Krange:\n",
    "            phi_flat(W, P, Q).backward(gradient=external_grad)\n",
    "            gradient=W.grad\n",
    "            alpha = torch.ones(m)*0.01\n",
    "            Gains = phi_flat(W, P, Q)\n",
    "            Gains_futur = phi_flat(W + alpha*gradient, P, Q)\n",
    "            for _ in LSrange:\n",
    "                mask = 0.0 + (Gains>Gains_futur)\n",
    "                alpha = 0.8*mask*alpha + 1.3*(1-mask)*alpha\n",
    "                Gains = torch.max(Gains, Gains_futur)\n",
    "                Gains_futur = phi_flat(W + alpha*gradient, P, Q)\n",
    "            W = projected_wiring(W + alpha*gradient).detach()\n",
    "            W.requires_grad=True\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of the Line Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p,q=0.39, 0.6\n",
    "P = p*PR +q*SR + (1-p-q)*I\n",
    "BoxProduct = non_local_boxes.evaluate.phi_flat\n",
    "\n",
    "m = non_local_boxes.evaluate.nb_columns\n",
    "LS_iterations = 10\n",
    "K_reset=5\n",
    "chi = 0.3\n",
    "\n",
    "W=line_search_with_resets(\n",
    "    P, \n",
    "    P, \n",
    "    LS_iterations=LS_iterations, \n",
    "    K_reset=K_reset, \n",
    "    chi=chi\n",
    "    )\n",
    "histogramLS = BoxProduct(W, P, P).tolist()\n",
    "\n",
    "plt.hist(histogramLS, bins=15, color='purple', \n",
    "         label=f\"\"\"Line Search with resets\\n($K_{{reset}}$={K_reset}, χ=${round(chi/10**int(np.log10(chi)-1))}\\cdot10^{{{int(np.log10(chi))}}}$, m=$10^{int(np.log10(m))}$, M={LS_iterations}).\"\"\")\n",
    "plt.xlim(0.3, 1)\n",
    "plt.ylim(0.9, m)\n",
    "plt.xlabel(\"$\\Phi(\\mathsf{W}_{{out}})$\", fontsize=13)\n",
    "plt.ylabel(\"Number of reruns\", fontsize=14)\n",
    "plt.yscale(\"log\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "# III. Comparison of GD and LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(histogramLS, bins=20, color='purple', label=\"Line Search with reruns ($K_{reset}$=\"+str(K_reset)+\", χ=\"+str(chi)+\", m=10^\"+str(int(np.log10(m)))+\", M=\"+str(LS_iterations)+\")\")\n",
    "plt.hist(histogramGD, bins=20, label=\"Gradient Descent (α=\"+str(alpha)+\", K=10^\"+str(int(np.log10(K)))+\", ε=10^\"+str(int(np.log10(epsilon)))+\", m=10^\"+str(int(np.log10(m)))+\")\")\n",
    "plt.xlim(0.3, 1)\n",
    "plt.ylim(0.9, m)\n",
    "plt.xlabel(\"$\\Phi(\\mathsf{W}_{{out}})$\")\n",
    "plt.ylabel(\"Number of reruns\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_boxes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
